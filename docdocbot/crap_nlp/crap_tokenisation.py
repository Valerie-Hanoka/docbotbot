#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
crap_tokenisation is part of the project docbotbot
Author: Val√©rie Hanoka

"""

def split_in_sentences(tweet):
    raise NotImplementedError

def split_in_tokens():
    raise NotImplementedError

def remove_stopwords(seq):
    raise NotImplementedError